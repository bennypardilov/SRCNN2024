{"cells":[{"cell_type":"markdown","metadata":{"id":"9wVPrWKQ3GWz"},"source":["# SRCNN2024 - Super Resolution Convolutional Neural Network"]},{"cell_type":"markdown","metadata":{"id":"8C2vWOPx5QfY"},"source":["## 1. Importing Libraries"]},{"cell_type":"markdown","metadata":{"id":"J8RAd1TW5VXB"},"source":["We import the necessary libraries for image processing and deep learning. This includes libraries for data manipulation (pandas, numpy), visualization (matplotlib), deep learning (torch), and image processing (PIL). These libraries will be used throughout the project for tasks such as loading and processing images, building neural networks, and optimizing model parameters."]},{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":16520,"status":"ok","timestamp":1714817524778,"user":{"displayName":"Binyamin Pardilov","userId":"08114672722662484580"},"user_tz":-180},"id":"MG1r9s7W2iRJ"},"outputs":[],"source":["import pandas\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","import torchvision.transforms.functional as TF\n","from torch.utils.data import TensorDataset, DataLoader\n","\n","import os\n","import sys\n","import glob\n","\n","import cv2\n","import tifffile\n","from PIL import Image\n","from scipy.signal import convolve2d\n","from skimage.util import random_noise"]},{"cell_type":"markdown","metadata":{"id":"qDrfqS0H6L3p"},"source":["## 2. Data"]},{"cell_type":"markdown","metadata":{"id":"M_1iIvak6QLo"},"source":["We begin by mounting Google Drive to access our project data stored there. This step allows seamless integration with Google Colab, facilitating data loading and manipulation directly from our Drive storage."]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":66424,"status":"ok","timestamp":1714817591191,"user":{"displayName":"Binyamin Pardilov","userId":"08114672722662484580"},"user_tz":-180},"id":"5-qZYJf98JAZ","outputId":"bf6460a2-d576-4f63-a2ab-3afa42740f63"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n"]}],"source":["from google.colab import drive\n","\n","# Set the path to the project directory in Google Drive\n","drive_path = '/content/gdrive/Shareddrives/Final Project/SRCNN2024/'\n","\n","# Mount Google Drive\n","drive.mount('/content/gdrive')"]},{"cell_type":"markdown","metadata":{"id":"aY78Qh1qvmxz"},"source":["### Cropping images"]},{"cell_type":"markdown","metadata":{"id":"T5GBpZKpv0FB"},"source":["To ensure uniformity across the dataset, we crop all images to a target size of $256\\times256$. This step is crucial for maintaining consistency in input dimensions, which is essential for deep learning models that require fixed-size inputs."]},{"cell_type":"code","execution_count":3,"metadata":{"id":"y1CZ6Ak5vl8T","executionInfo":{"status":"ok","timestamp":1714817591191,"user_tz":-180,"elapsed":5,"user":{"displayName":"Binyamin Pardilov","userId":"08114672722662484580"}}},"outputs":[],"source":["def crop_images(input_folder, output_folder):\n","    # Create output folder if it doesn't exist\n","    if not os.path.exists(output_folder):\n","        os.makedirs(output_folder)\n","\n","    # Iterate through all directories and files in the input folder\n","    for root, dirs, files in os.walk(input_folder):\n","        for filename in files:\n","            if filename.endswith('.tif'):\n","                # Open image\n","                image_path = os.path.join(root, filename)\n","                img = Image.open(image_path)\n","\n","                # Crop image\n","                cropped_img = img.crop((0, 0, 256, 256))\n","\n","                # Save cropped image\n","                output_path = os.path.join(output_folder, filename)\n","                cropped_img.save(output_path)\n","\n","# input_folder = drive_path + \"data/Images/\"\n","# output_folder = drive_path + \"data/HR_data/\"\n","# crop_images(input_folder, output_folder)"]},{"cell_type":"markdown","metadata":{"id":"WqEzImQdwOgT"},"source":["By cropping the images to a consistent size, we ensure that all samples in the dataset have the same dimensions, simplifying the preprocessing pipeline and ensuring compatibility with downstream tasks such as model training and evaluation."]},{"cell_type":"markdown","metadata":{"id":"cNUTolSBNeLz"},"source":["### Image downsampling"]},{"cell_type":"markdown","metadata":{"id":"oKfE2F1kf341"},"source":["We define a function `downsample` to perform downsampling on an image. This function takes parameters such as kernel size, scale factor, and noise statistics to generate a downsampled low-resolution (LR) version of the input image. Additionally, we provide a utility function `create_LR_images` to apply the downsampling function to all images in a specified folder and save the resulting LR images to an output folder.\n","\n","The downsampling operation involves convolving the image with a downsampling kernel followed by downsampling by a specified factor. Mathematically, given an input image $I_{\\text{HR}}$, a downsampling kernel $K$, the downsampling operation with noise can be represented as:\n","\n","\\begin{align}\n","  I_{\\text{LR}} = (I_{\\text{HR}} * K) + N\n","\\end{align}\n","\n","where $I_{\\text{LR}}$ is the downsampled image, $*$ denotes the convolution operation, and $N$ represents the noise added to the downsampled image.\n"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"8-qCRQDtNdhz","executionInfo":{"status":"ok","timestamp":1714817591191,"user_tz":-180,"elapsed":4,"user":{"displayName":"Binyamin Pardilov","userId":"08114672722662484580"}}},"outputs":[],"source":["def downsample(image, kernel_size=(5, 5), scale_factor=2, noise_mean=0, noise_std=1):\n","    # Define a downsampling kernel\n","    kernel = np.ones(kernel_size, dtype=np.float64) / (kernel_size[0] * kernel_size[1])\n","\n","    # Initialize an empty array to store the downsampled images for each channel\n","    LR_images = []\n","\n","    # Iterate over each channel\n","    for channel in range(image.shape[2]):\n","        # Convolve the channel with the downsampling kernel\n","        downsampled_channel = convolve2d(image[:, :, channel], kernel, mode='same')[::scale_factor, ::scale_factor]\n","\n","        # Add noise to the downsampled channel\n","        noise = random_noise(np.zeros(downsampled_channel.shape), mean=noise_mean, var=noise_std**2)\n","        LR_channel = downsampled_channel + noise\n","\n","        # Clip the values to be within [0, 255]\n","        LR_channel = np.clip(LR_channel, 0, 255)\n","\n","        # Append the downsampled channel to the list\n","        LR_images.append(LR_channel)\n","\n","    # Convert the list of downsampled channels to an array\n","    LR_image = np.stack(LR_images, axis=-1)\n","\n","    return LR_image\n","\n","\n","def create_LR_images(input_folder, output_folder):\n","    # Create output folder if it doesn't exist\n","    if not os.path.exists(output_folder):\n","        os.makedirs(output_folder)\n","\n","    # Iterate through all images in the input folder\n","    for filename in os.listdir(input_folder):\n","        if filename.endswith('.tif'):\n","            # Read the image\n","            image_path = os.path.join(input_folder, filename)\n","            image = tifffile.imread(image_path)\n","\n","            # Create the LR image\n","            LR_image = downsample(image)\n","\n","            # Save the LR image to the output folder\n","            output_path = os.path.join(output_folder, filename)\n","            tifffile.imwrite(output_path, LR_image.astype(np.uint8))\n","\n","\n","input_folder = drive_path + \"data/HR_data/\"\n","output_folder = drive_path + \"data/LR_data/\"\n","# create_LR_images(input_folder, output_folder)"]},{"cell_type":"markdown","metadata":{"id":"tlS7kdt09p7x"},"source":["### Loading the data"]},{"cell_type":"markdown","metadata":{"id":"8Fpb7fRrJAkX"},"source":["To facilitate image loading and preprocessing, we define a function `load_images`. This function loads images from a specified directory, performs preprocessing steps such as removing the alpha channel and normalizing pixel values, and constructs a data array with all images. This array ensures uniform dimensions across all images, which is crucial for model training."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fGOJ4EEM_TUj"},"outputs":[],"source":["def load_images(path):\n","    images = []\n","    # Get sorted list of image file paths\n","    image_paths = sorted(glob.glob(path))\n","\n","    for file in image_paths:\n","        img = np.asarray(Image.open(file))      # read the image as a numpy array\n","        img = img[:, :, :3]                     # remove the alpha channel if exists\n","\n","        # Normalize the image to have a mean of 0 and standard deviation of 1\n","        img = (img / 255.0) - 0.5\n","\n","        images.append(img)\n","\n","    # Find the maximum dimensions among all images\n","    max_height = max(img.shape[0] for img in images)\n","    max_width = max(img.shape[1] for img in images)\n","\n","    # Prepare data array with the maximum dimensions\n","    data = np.zeros((len(images), max_height, max_width, 3))\n","\n","    # Populate data array with images\n","    for i, img in enumerate(images):\n","        h, w, _ = img.shape\n","        data[i, :h, :w] = img\n","\n","    return data\n","\n","# np.save(drive_path + \"data/HR_data.npy\", load_images(drive_path + \"data/HR_data/*.tif\"))\n","# np.save(drive_path + \"data/LR_data.npy\", load_images(drive_path + \"data/LR_data/*.tif\"))\n","\n","HR_data = np.load(drive_path + \"data/HR_data.npy\")\n","LR_data = np.load(drive_path + \"data/LR_data.npy\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZOm24vDWcjEK"},"outputs":[],"source":["print(HR_data.shape, LR_data.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"L2H49DUFEksw"},"outputs":[],"source":["# Plotting HR and LR images side by side\n","fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n","\n","# Plot HR image\n","axes[0].imshow(HR_data[1000] + 0.5)\n","axes[0].set_title(f\"HR Image\\nDimensions: {HR_data.shape[1:3]}\")\n","\n","# Plot LR image\n","axes[1].imshow(LR_data[1000] + 0.5)\n","axes[1].set_title(f\"LR Image\\nDimensions: {LR_data.shape[1:3]}\")\n","\n","plt.show()"]},{"cell_type":"markdown","source":["### Shuffle data, convert to tensor"],"metadata":{"id":"G1-ypl3z-mUw"}},{"cell_type":"code","source":["# Generate shuffled indices\n","indices = np.arange(HR_data.shape[0])\n","np.random.shuffle(indices)\n","\n","# Rearrange both arrays using shuffled indices\n","LR_data = LR_data[indices]\n","HR_data = HR_data[indices]"],"metadata":{"id":"WgQGYAAM-srH"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"G-Dm_eJCx1ey"},"outputs":[],"source":["# Convert LR_data and HR_data to PyTorch tensor\n","LR_data = torch.tensor(LR_data).permute(0, 3, 1, 2).float()\n","HR_data = torch.tensor(HR_data).permute(0, 3, 1, 2).float()\n","\n","print(HR_data.shape, LR_data.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CqIRdWnZeBFw"},"outputs":[],"source":["train_size = int(0.8 * HR_data.shape[0])\n","valid_size = train_size + int(0.1 * HR_data.shape[0])\n","test_size = valid_size + int(0.1 * HR_data.shape[0])\n","\n","print(\"train size:\", train_size, \"valid size:\", valid_size - train_size, \"test size:\", test_size - valid_size)\n","\n","LR_train_data = LR_data[:train_size]\n","HR_train_data = HR_data[:train_size]\n","\n","LR_valid_data = LR_data[train_size:valid_size]\n","HR_valid_data = HR_data[train_size:valid_size]\n","\n","LR_test_data = LR_data[valid_size:]\n","HR_test_data = HR_data[valid_size:]"]},{"cell_type":"markdown","metadata":{"id":"eCOzB40IZy64"},"source":["## 3. Readymade code for upsampling"]},{"cell_type":"markdown","metadata":{"id":"oyY48ulv84FD"},"source":["**Pixel Shuffle dosen't work!**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Y37O-L9IZ0Bp"},"outputs":[],"source":["# Define Transposed Convolution\n","def transposed_conv(input, k, s):\n","    return nn.ConvTranspose2d(in_channels=3, out_channels=3, kernel_size=k, stride=s, padding=1)(input)\n","\n","# Define Pixel Shuffle Upscaling\n","def pixel_shuffle(input, uf):\n","    return nn.PixelShuffle(upscale_factor=uf)(input)\n","\n","# Define Adaptive Group Upscaling\n","def adaptive_group(input, os):\n","    return nn.AdaptiveAvgPool2d(output_size=os)(input)\n","\n","# Define Sub-pixel Upscaling\n","def sub_pixel(input, oc, uf):\n","    return nn.Sequential(nn.Conv2d(in_channels=3, out_channels=oc, kernel_size=3, stride=1, padding=1),\n","                         nn.PixelShuffle(upscale_factor=uf))(input)\n","\n","# Define Bicubic Interpolation (Bicubic Upscaling)\n","def bicubic(input, os):\n","    return F.interpolate(input, size=os, mode='bicubic')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IacG1NwWaDFp"},"outputs":[],"source":["# Apply each function for upscaling by factor 2\n","transposed_conv_x2 = transposed_conv(LR_data, k=4, s=2)\n","print(transposed_conv_x2.size())\n","adaptive_group_x2 = adaptive_group(LR_data, os=(252, 252))\n","print(adaptive_group_x2.size())\n","sub_pixel_x2 = sub_pixel(LR_data, oc=12, uf=2)\n","print(sub_pixel_x2.size())\n","bicubic_x2 = bicubic(LR_data, os=(252, 252))\n","print(bicubic_x2.size())\n","\n","transposed_conv_x3 = transposed_conv(LR_data, k=6, s=3)\n","print(transposed_conv_x3.size())\n","adaptive_group_x3 = adaptive_group(LR_data, os=(378, 378))\n","print(adaptive_group_x3.size())\n","sub_pixel_x3 = sub_pixel(LR_data, oc=27, uf=3)\n","print(sub_pixel_x3.size())\n","bicubic_x3 = bicubic(LR_data, os=(378, 378))\n","print(bicubic_x3.size())\n","\n","transposed_conv_x4 = transposed_conv(LR_data, k=8, s=4)\n","print(transposed_conv_x4.size())\n","bicubic_x4 = bicubic(LR_data, os=(504, 504))\n","print(bicubic_x4.size())\n","\n","# Plot images for visual comparison\n","fig, axes = plt.subplots(3, 4, figsize=(12, 9))\n","\n","# Factor 2\n","axes[0, 0].imshow(transposed_conv_x2.permute(0, 2, 3, 1).detach().numpy()[0] + 0.5)\n","axes[0, 0].set_title(\"Transposed Convolution x2\")\n","axes[0, 1].imshow(adaptive_group_x2.permute(0, 2, 3, 1).detach().numpy()[0] + 0.5)\n","axes[0, 1].set_title(\"Adaptive Group x2\")\n","axes[0, 2].imshow(sub_pixel_x2.permute(0, 2, 3, 1).detach().numpy()[0] + 0.5)\n","axes[0, 2].set_title(\"Sub-pixel x2\")\n","axes[0, 3].imshow(bicubic_x2.permute(0, 2, 3, 1).detach().numpy()[0] + 0.5)\n","axes[0, 3].set_title(\"Bicubic x2\")\n","\n","# Factor 3\n","axes[1, 0].imshow(transposed_conv_x3.permute(0, 2, 3, 1).detach().numpy()[0] + 0.5)\n","axes[1, 0].set_title(\"Transposed Convolution x3\")\n","axes[1, 1].imshow(adaptive_group_x3.permute(0, 2, 3, 1).detach().numpy()[0] + 0.5)\n","axes[1, 1].set_title(\"Bicubic x3\")\n","axes[1, 2].imshow(sub_pixel_x3.permute(0, 2, 3, 1).detach().numpy()[0] + 0.5)\n","axes[1, 3].imshow(bicubic_x3.permute(0, 2, 3, 1).detach().numpy()[0] + 0.5)\n","\n","# Factor 4\n","axes[2, 0].imshow(transposed_conv_x4.permute(0, 2, 3, 1).detach().numpy()[0] + 0.5)\n","axes[2, 0].set_title(\"Transposed Convolution x4\")\n","axes[2, 1].imshow(bicubic_x4.permute(0, 2, 3, 1).detach().numpy()[0] + 0.5)\n","axes[2, 1].set_title(\"Bicubic x4\")\n","axes[2, 2].axis('off')  # No result for adaptive_group_x4\n","axes[2, 3].axis('off')  # No result for sub_pixel_x4\n","\n","plt.tight_layout()\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"N8JChSAaBIpg"},"source":["## CNN - U-Net based architecture"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ATAyMJffVNua"},"outputs":[],"source":["class DoubleConv(nn.Module):\n","    def __init__(self, in_channels, out_channels):\n","        super().__init__()\n","        self.conv_op = nn.Sequential(\n","            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n","            nn.ReLU(inplace=True)\n","        )\n","\n","    def forward(self, x):\n","        return self.conv_op(x)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lqH0RClSVK6W"},"outputs":[],"source":["class DownSample(nn.Module):\n","    def __init__(self, in_channels, out_channels):\n","        super().__init__()\n","        self.conv = DoubleConv(in_channels, out_channels)\n","        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n","\n","    def forward(self, x):\n","        down = self.conv(x)\n","        p = self.pool(down)\n","\n","        return down, p"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-3XbFx-NXRhu"},"outputs":[],"source":["class UpSample(nn.Module):\n","    def __init__(self, in_channels, out_channels):\n","        super().__init__()\n","        self.up = nn.ConvTranspose2d(in_channels, in_channels // 2, kernel_size=2, stride=2)\n","        self.conv = DoubleConv(in_channels, out_channels)\n","\n","    def forward(self, x1, x2):\n","        x1 = self.up(x1)\n","        x = torch.cat([x1, x2], dim=1)\n","        return self.conv(x)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EU6SqYGwYIv3"},"outputs":[],"source":["class UNet(nn.Module):\n","    def __init__(self, in_channels, out_channels):\n","        super().__init__()\n","\n","        self.down_conv_1 = DownSample(in_channels, 64)\n","        self.down_conv_2 = DownSample(64, 128)\n","        self.down_conv_3 = DownSample(128, 256)\n","        self.down_conv_4 = DownSample(256, 512)\n","\n","        self.bottle_neck = DoubleConv(512, 1024)\n","\n","        self.up_conv_1 = UpSample(1024, 512)\n","        self.up_conv_2 = UpSample(512, 256)\n","        self.up_conv_3 = UpSample(256, 128)\n","        self.up_conv_4 = UpSample(128, 64)\n","\n","        self.upsample = nn.Upsample(scale_factor=2, mode='bicubic')\n","\n","        self.out = nn.Conv2d(64, out_channels, kernel_size=1)\n","\n","    def forward(self, x):\n","        down_1, p1 = self.down_conv_1(x)\n","        down_2, p2 = self.down_conv_2(p1)\n","        down_3, p3 = self.down_conv_3(p2)\n","        down_4, p4 = self.down_conv_4(p3)\n","\n","        b = self.bottle_neck(p4)\n","\n","        up_1 = self.up_conv_1(b, down_4)\n","        up_2 = self.up_conv_2(up_1, down_3)\n","        up_3 = self.up_conv_3(up_2, down_2)\n","        up_4 = self.up_conv_4(up_3, down_1)\n","\n","        upsample = self.upsample(up_4)\n","\n","        out = self.out(upsample)\n","\n","        return out"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5KkmgPVdbtAB"},"outputs":[],"source":["input_image = torch.rand((5, 3, 128, 128))\n","model = UNet(3, 3)\n","output = model(input_image)\n","print(output.shape)"]},{"cell_type":"markdown","metadata":{"id":"SjjZxy6rL510"},"source":["## Train"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LPeH12Tuf_zn"},"outputs":[],"source":["LEARNING_RATE = 5e-4\n","BATCH_SIZE = 18\n","EPOCHS = 20\n","\n","DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","\n","train_dataset = TensorDataset(LR_train_data, HR_train_data)\n","valid_dataset = TensorDataset(LR_valid_data, HR_valid_data)\n","\n","train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n","valid_loader = DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=True)\n","\n","model = UNet(3, 3).to(DEVICE)\n","optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n","criterion = nn.MSELoss()\n","scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.5)\n","\n","train_loss, valid_loss = 0.0, 0.0\n","\n","for epoch in range(EPOCHS):\n","    model.train()\n","    train_running_loss = 0\n","\n","    for idx, LR_HR in enumerate(train_loader):\n","        LR = LR_HR[0].float().to(DEVICE)\n","        HR = LR_HR[1].float().to(DEVICE)\n","\n","        pred = model(LR)\n","        optimizer.zero_grad()\n","\n","        loss = criterion(pred, HR)\n","        train_running_loss += loss.item()\n","\n","        loss.backward()\n","        optimizer.step()\n","\n","    train_loss = train_running_loss / (idx + 1)\n","\n","    model.eval()\n","    valid_running_loss = 0\n","    with torch.no_grad():\n","        for idx, LR_HR in enumerate(valid_loader):\n","            LR = LR_HR[0].float().to(DEVICE)\n","            HR = LR_HR[1].float().to(DEVICE)\n","\n","            pred = model(LR)\n","            loss = criterion(pred, HR)\n","            valid_running_loss += loss.item()\n","\n","        valid_loss = train_running_loss / (idx + 1)\n","\n","    scheduler.step()\n","\n","    print(\"-\"*30)\n","    print(f\"Train loss EPOCH {epoch + 1}: {train_loss:.4f}\")\n","    print(f\"Valid loss EPOCH {epoch + 1}: {valid_loss:.4f}\")\n","    print(\"-\"*30)\n"]},{"cell_type":"markdown","source":["## Save the model"],"metadata":{"id":"eZVByX27BY5J"}},{"cell_type":"code","source":["torch.save(model.state_dict(), drive_path + \"best_model.pk\")"],"metadata":{"id":"ba2riKw9Ba94"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Test"],"metadata":{"id":"Lkpn3pN3BbwC"}},{"cell_type":"code","source":["# Choose an LR image from the training set\n","lr_image = LR_train_data[30].to(DEVICE)  # Change the index as needed\n","\n","# Bicubic interpolation\n","# bicubic_x2 = bicubic(LR_train_data, os=(256, 256))\n","bicubic_output = bicubic_x2.permute(0, 2, 3, 1).detach().numpy()[30] + 0.5\n","\n","# Pass the LR image through the model to get the SR output\n","model.eval()\n","with torch.no_grad():\n","    sr_output = model(lr_image.unsqueeze(0)).cpu()  # Unsqueeze to add batch dimension and move to CPU\n","\n","# Convert tensors to numpy arrays and remove batch dimension\n","lr_image = lr_image.permute(1, 2, 0).cpu().numpy() + 0.5\n","sr_output = sr_output.squeeze().permute(1, 2, 0).numpy() + 0.5\n","\n","\n","# Plot the LR image, SR output, HR image, and Bicubic interpolation\n","plt.figure(figsize=(20, 5))\n","\n","plt.subplot(1, 4, 1)\n","plt.imshow(lr_image)\n","plt.title('LR Image')\n","plt.axis('off')\n","\n","plt.subplot(1, 4, 2)\n","plt.imshow(bicubic_output)\n","plt.title('Bicubic Interpolation')\n","plt.axis('off')\n","\n","plt.subplot(1, 4, 3)\n","plt.imshow(sr_output)\n","plt.title('SR output')\n","plt.axis('off')\n","\n","plt.subplot(1, 4, 4)\n","plt.imshow(HR_train_data[30].permute(1, 2, 0).cpu().numpy() + 0.5)\n","plt.title('HR Image')\n","plt.axis('off')\n","\n","plt.show()"],"metadata":{"id":"wXcieDO-aeIO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Choose an LR image from the training set\n","lr_image = LR_test_data[15].to(DEVICE)  # Change the index as needed\n","\n","# Bicubic interpolation\n","bicubic_x2_test = bicubic(LR_test_data, os=(256, 256))\n","bicubic_output = bicubic_x2_test.permute(0, 2, 3, 1).detach().numpy()[15] + 0.5\n","\n","# Pass the LR image through the model to get the SR output\n","model.eval()\n","with torch.no_grad():\n","    sr_output = model(lr_image.unsqueeze(0)).cpu()  # Unsqueeze to add batch dimension and move to CPU\n","\n","# Convert tensors to numpy arrays and remove batch dimension\n","lr_image = lr_image.permute(1, 2, 0).cpu().numpy() + 0.5\n","sr_output = sr_output.squeeze().permute(1, 2, 0).numpy() + 0.5\n","\n","\n","# Plot the LR image, SR output, HR image, and Bicubic interpolation\n","plt.figure(figsize=(20, 5))\n","\n","plt.subplot(1, 4, 1)\n","plt.imshow(lr_image)\n","plt.title('LR Image')\n","plt.axis('off')\n","\n","plt.subplot(1, 4, 2)\n","plt.imshow(bicubic_output)\n","plt.title('Bicubic Interpolation')\n","plt.axis('off')\n","\n","plt.subplot(1, 4, 3)\n","plt.imshow(sr_output)\n","plt.title('SR output')\n","plt.axis('off')\n","\n","plt.subplot(1, 4, 4)\n","plt.imshow(HR_test_data[15].permute(1, 2, 0).cpu().numpy() + 0.5)\n","plt.title('HR Image')\n","plt.axis('off')\n","\n","plt.show()"],"metadata":{"id":"-HRuwQtrko5L"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["eCOzB40IZy64"],"gpuType":"T4","machine_shape":"hm","provenance":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
